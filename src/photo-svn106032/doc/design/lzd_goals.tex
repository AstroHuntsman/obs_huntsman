	The goal of the level zero implementation of the photometric pipeline
is to produce a system capable of reducing imaging data from the 2.5m
photometric camera to a form from which the core scientific projects
(outlined in the Principles of Operations) can be undertaken. The
data products produced by the level zero pipeline are:

\begin{itemize}

\item Bias vectors, one per CCD

\item Flatfield vectors, one per frame and color

\item Calibration parameters, one per frame and color.  These include the
sky, slope of the sky in the row direction left and right bias correction,
number of counts a 20th magnitude star would deposit in some radius (and
the error in that number), and the double Gaussian psf fit.

\item A flattened, bias corrected set of image frames
(2048$\times$1489 pixels).

\item A 4$\times$4 pixel binned version of the corrected data.

\item A set of image frames that contain the value of the noise in
each pixel of the corrected frames

\item A set of mask frames that identify the position of objects and
defects across a corrected frame

\item A structure containing the pixel statistics of each frame and the number
of stars, galaxies, and defects identified.

\item A combined list of objects detected in each of the five colors
together with photometric parameters measured for each object and cut
out image regions around each object.
\end{itemize}


\section{Level Zero Performance}

The design specifications of level zero and the degree to which the
current implementation meets these goals are described below.

\begin{itemize}

\item Identify all sources down to the 5 $\sigma$ level.

 Here we defined the signal to noise ratio as following.

\[
(S/N) = \frac{n_{obj}\times(e/DN)}{\sqrt{(n_{obj}+n_{sky}\times n_{eff})\times(e/DN)+R^2}},
\]

\noindent
where $n_{obj}$ is an integrated object DN counts, $n_{sky}$ is background 
DN counts for 1 pixel, $n_{eff}$ is the effective number of pixels for an 
stellar object, $R$ is readout noise in electron, and (e/DN) is DN to electron 
conversion factor. We used 28 for $n_{eff}$ and 7 for $R$. Other parameters 
are given in the simulation data.

 The magnitude which corresponds to $(S/N)=5$ will be 22.43, 23.36, 23.05, 
22.45, and 20.92 mag for filters $u^\prime$, $g^\prime$, $r^\prime$, 
$i^\prime$ and $z^\prime$ respectively.

\begin{figure}
\plottwo{u_hist.ps}{g_hist.ps}
\plottwo{r_hist.ps}{i_hist.ps}
\plotone{z_hist.ps}{}
\caption{The results of object finding. The solid histogram shows the number ofobjects in the simulation catalog. The dashed histogram shows the number of
the objects detected and matched with the objects in the simulation
catalog. The dot-dashed histogram shows the number of the objects detected but
not matched with the objects in the simulation catalog. The numbers in
the title correspond to the sum of these three histograms. The dotted vertical
line shows the magnitude which corresponds to $(S/N)=5$.}
\end{figure}

 The above figure shows the results of object finding.  At 5 sigma, we find 
about 20\% of the objects in u', 15\% in g', 20\% in r', 10\% in i', and 10\% 
in z'.  Most of the objects are found one magnitude brighter than the 5 sigma
goal.

\item Identify and extract known sources from catalogs of local
galaxies and bright stars.

Identification and extraction of known sources (e.g.\ galaxies with
diameters $>$1 arcmin, planetary nebulae and very bright stars) has
not been implemented for level zero.

\item Perform star/galaxy separation down to 20th mag.
($g^\prime$) with no more than 5\% stellar contamination and 95\%
completion. 

We expect that the survey star/galaxy classification to faint limits will
make use of a number of different parameters at different magnitudes; for
example a Bayesian comparison the quality of PSF {\it v.\/} `galaxy' fits
to the radial light profile. For level zero no such system has been created.

\begin{figure}
\plottwoNR{starVgalaxy1.eps}{starVgalaxy2.eps}
\caption{Stars and galaxies, as specified in the input catalogue, plotted
using input ($r'$) magnitudes and parameters measured by level zero
measureObjects. For the $g'$ and $r'$ plots, the adopted classification line
is shown.}
\end{figure}
Fortunately, we are able to meet the goals of level zero using far less
sophisticated schemes. The figure on the left uses a classifier based on
$\sqrt{}(\sigma^2_{xx} + \sigma^2_{yy})$; the ones on the right are based
on $\hbox{peak counts}/\sqrt{N_{\hbox{pix}}}$. The `classifier' lines are
based on an examination of the figures, and have no objective value. They
are provided solely to allow us to calculate the next set of figures, giving
the error rates for classifying using such simple parameter cuts.

\begin{figure}
\plottwoNR{starVgalaxyComplete1.eps}{starVgalaxyComplete2.eps}
\caption{For the $r'$ classifiers presented above, we show the rates of
classification of stars as stars; stars as galaxies, galaxies as stars,
and galaxies as galaxies.}
\end{figure}

An inspection of the figures reveals that even these simple-minded classifiers
reach the survey goals of 95\% completion and 5\% errors to an $r'$ 
magnitude of about 19.5;
as galaxies are significantly brighter at $r'$ than $g'$ we have reached
the level zero goal.

\item Derive stellar magnitudes to an accuracy of 2\% at 19.7, 
20.6, 20.3, 19.7 and 18.2 mag. for filters $u^\prime$, $g^\prime$,
$r^\prime$, $i^\prime$ and $z^\prime$ respectively.

 There are 5 magnitudes currently measured in the pipeline. The following is 
the statistics of the difference of the simulation catalog magnitude and 
the measured magnitude using the objects down to the magnitude specified 
above. The numbers in parentheses are offset and standard variation in the 
difference.

\begin{center}
\begin{tabular}{c|ccccc}
\hline
band       & totalCounts      & fibreCounts      & apCounts         & psfCounts       & fiso \\
\hline\hline					                    
$u^\prime$ & ($-0.028,0.026$) & ($-0.275,0.012$) & ($-0.019,0.016$) & ($0.186,0.019$) & ($+0.168,0.163$) \\
$g^\prime$ & ($-0.050,0.061$) & ($-0.261,0.024$) & ($+0.005,0.046$) & ($0.290,0.074$) & ($-0.033,0.055$) \\
$r^\prime$ & ($-0.052,0.068$) & ($-0.259,0.027$) & ($-0.003,0.053$) & ($0.319,0.084$) & ($-0.038,0.048$) \\
$i^\prime$ & ($-0.022,0.053$) & ($-0.266,0.015$) & ($-0.020,0.050$) & ($0.285,0.105$) & ($-0.038,0.039$) \\
$z^\prime$ & ($-0.032,0.046$) & ($-0.266,0.012$) & ($-0.003,0.031$) & ($0.280,0.101$) & ($-0.038,0.036$) \\
\hline
\end{tabular}
\end{center}

 The fibreCounts is the only magnitude which satisfies the
requirement, that is 2\%. Since fibreCounts is the magnitude in 3
arcsecond diameter, however, there is a large offset. Instead of 3
arcsecond diameter, apCounts is measured by 6 arcsecond diameter. This
shows symmetric distribution around the zero point but the scatter
becomes large for faint stars. Both totalCounts and fiso become
fainter for the objects whose simulation catalog magnitudes is fainter
than one magnitude above from the specified magnitude. psfCounts shows
the trend to become brighter for the fainter objects.

\input {photfigs.tex}

\item Measure sufficient shape parameters to meet the minimal
requirements for galaxy selection by Survey Strategy.

The current galaxy selection criteria used by Survey Strategy for
selecting the spectroscopic sample is based entirely on photometric
parameters. Selection is derived from a combination of a total
magnitude and a surface brightness limit. There are no requirements
for any shape information. The level zero pipeline outputs a total
magnitude (Petrosian magnitude) and an aperture magnitude based on the
light going down a 3 arcsec fiber, positioned at the intensity weighted
centroid. In addition to this the pipeline outputs the following shape
and positional parameters for future use by Survey Strategy.

\begin{tabular}{ll}
obj--$>$rowc    &   Centroid of an object defined by the intensity weighted 1st moments \\
obj--$>$colc    & \\ 
obj--$>$npix    &  Number of pixels\\ 
obj--$>$nsat    &  Number of pixels flagged as saturated in an image\\
obj--$>$npeaks  &  Number of peaks within an image\\
obj--$>$w\_xx   &   Intensity weighted second moments\\
obj--$>$w\_yy   & \\ 
obj--$>$w\_xy   &\\
obj--$>$psf\_xc &   Centroid defined by the psf\\
obj--$>$psf\_yc &\\    
obj--$>$psf\_amp &  Amplitude of fitted psf\\
obj--$>$psf\_chisq &Goodness of fit to psf\\
obj--$>$majaxis   &Position angle of the semimajor axis\\
obj--$>$eccen     &Eccentricity of an object\\
obj--$>$gauss\_xc & Centroid from the fitting of a Gaussian to the light profile\\
obj--$>$gauss\_yc  &\\
obj--$>$gauss\_wx  &FWHM of a Gaussian fitted to the light profile\\
obj--$>$gauss\_wy  &\\ 
obj--$>$profRad[N] &  Array of circular aperture annuli\\
obj--$>$profMean[N]&  Mean pixel value within the annuli\\
obj--$>$profMed[N] &  Median pixel value within the annuli\\
obj--$>$profStdev[N]& StdDev of pixel values within the annuli\\
obj--$>$profRatio[N]& Ratio of semimajor to semiminor axes of ellipse fits\\
obj--$>$profAngle[N]& Position angle of ellipse fits\\
obj--$>$Cin         &     Concentration index\\
obj--$>$SB          &     Surface Brightness\\
\end{tabular}

\item Excise no more than 2\% of the image region in the form of atlas
images.

Objects detected above the sky are grown to a threshold of one sigma
per pixel above the sky. Rectangular regions, that encompass the
pixels associated with an object, are extracted as atlas images. The
table below shows a comparison of the input and output volumes
produced by the photometric pipeline, on a frame by frame basis.
Column one gives the frame number, column two the total number of
pixels input from each 5 color frame, column three the number of
pixels in the 5 color atlas images and column four is the number of
pixels flagged as containing an object.

The resulting atlas images have a median size of 7\% of the input
frames. For linear structures (e.g.\ satellite trails, diffraction
spikes) this can rise to $>100\%$ of the input size. The region of a
frame occupied by object pixels has a median of about 3\%. 50\% of
each atlas image is, therefore, sky. Without compression the goals of
the level zero design are unfeasible (i.e.\ objects occupy more than
2\% of the input volume). To remedy this situation, we can
output compressed images, allow the atlas images to be non-rectangular,
discard some of the objects without making atlas images, or increase
the amount of data that the pipeline is allowed to output.

\begin{tabular}{l|r|rr|rr|rr}
\multicolumn{1}{c|}{Frame} & \multicolumn{1}{c|}{Input Size} & 
\multicolumn{2}{c|}{Atlas Images} & \multicolumn{2}{c|}{Object Mask} \\
      &  \multicolumn{1}{c|}{(pixels)}  & \multicolumn{2}{c|}{(pixels)} & 
\multicolumn{2}{c|}{(pixels)} \\
\hline
\hline
0 & 15247360 & 22170190 & 154\% & 1820000 & 12\% \\
1 & 15247360 & 713015 & 5\% & 325645 & 2\% \\
2 & 15247360 & 5082235 & 33\% & 1264885 & 8\% \\
3 & 15247360 & 837890 & 5\% & 401935 & 3\% \\
4 & 15247360 & 1045445 & 7\% & 494485 & 3\%  \\
5 & 15247360 & 1197570 & 8\% & 422525 & 3\%   \\
6 & 15247360 & 700530 & 5\%  & 309865 & 2\%   \\
7 & 15247360 & 4626955 & 30\% & 1318315 & 8\%  \\
8 & 15247360 & 1059670 & 7\% &456740 & 3\%   \\
\end{tabular}

\item Total size of the output parameter lists and atlas images must
be less than 5\% of the size of the input data.

Each OBJECT1 structure stores information for 58 parameters, 6 arrays
of 10 values for the isophotal/aperture photometry, and a set of
atlas images and masks. Excluding the REGION and MASKS this comprises
107 floating point numbers, 6 integers and 5 longs or a total of 680
bytes per object per color. The average number of detected objects per
frame is approximately 300. Therefore, for 5 colors the output
parameter list is 1.02 MB. This compares with an input volume of 30.72
MB for the 5 color frames with 1500$\times$2048 pixels. The parameter list
alone is 3.3\% of the input volume. If we include the atlas
images discussed above then the total output is 28\% of
the input volume. Excluding frame 1 (with the extended linear feature) this
falls to 16\%. The frequency of large linear structures within the survey
field will impact heavily on the output volume of the pipeline.

The OBJECT1 structure is inefficient in its storage of the parameters.
Values are typically stored as floats when the precision we can
measure them to only justifies a 2 byte number (e.g.\ we can measure
centroids to, at best, a 10th of a pixel). By converting the
parameters to the number of bytes their accuracy requires, the size can
be reduced to 230 bytes per object per color.

\item Reduce a 2.75 hour observation data tape in less than 12 hours
on a 330 Mbyte, 150 MIP computer.

We were not presented with a 2.75 hour data tape.  We instead processed
ten simulated images of the sky.  Each image took about 20 CPU minutes
to process on a 150 MIP processor.  The real time varies, but is about
half an hour per image.

A 2.75 hour data tape would have about 260 frames of 1354 rows and five
colors.  With the level zero system, it would take about 130 hours to
reduce this data (off by about a factor of ten).  Not much work has been
done to optimize the code (this code was in fact compiled debug), but
is unknown to what extent this CPU time can be reduced without
algorithmic changes.

The photometric pipeline does run within the 330 Mbytes specified in
the level zero design document, but level one will probably have to reduce
the memory requirements further.  About 140 Mbytes of memory are allocated
on startup.  This includes (in round numbers) 10Mbytes for the executable,
$5 x 6 = 30$ Mbytes for raw images, $5 X 6 = 30$ Mbytes read-in buffer,
$5 x 6 = 30$ Mbytes for the corrected frames, $5 x 3 = 15$ Mbytes for the
noise regions, $5 x 3 = 15$ Mbytes for the masks, and a 12 Megabyte floating
point scratch space region.  During operation, the photometric pipeline
allocates about 80 Mbytes of space.  Of this, 30 Mbytes is for sky frames
and about 30 Mbytes is for scratch space in the object finder.  The rest
is primarily for object lists.  For frames with nasty things (like the
frame with the two crossing satellite trails), the object lists can require
100 Mbytes of additional space.  Something must be implemented in level one to
limit the size of the atlas images cut out.  The worst case currently is
a large atlas image (a satellite trail that goes from one corner to the
opposite corner of the frame will produce an atlas image the size of the
frame) that is deblended into many things, thus producing many copies of the
same pixels.  The atlas images, remember, are four bytes per pixel, since
they include one byte for the noise and one byte for the mask.

In addition, there is memory fragmentation that occurs each time a frame
goes through the pipeline.  On the eight frames studied, there was on
average 15 Mbytes of fragmentation per frame, but the statistics were not
good enough to give a good number or to determine if there was an
asymtotic approach to a maximum size.  This can be solved by clever memory
allocation, memory compression, or by periodically stopping the process
and starting over with a new batch of memory.  This will not be solved
in level zero.

\item Run the pipeline for a 24 hour period without crashing.

We did not run the pipeline on data sets larger than ten images, and
it is clear that without better memory management, this goal will not
be achieved.

\item Exclude no more than 5\% of the catalog area due to defects or
bright objects.

The largest simulation available to the frames pipeline is equivalent
to approximately 5 minutes of observing time ($>800$MB of data). The
distribution of bright sources is therefore somewhat artificial (i.e.\
we impose bright stars and satellite trails on a frame without taking
into account their actual frequency). Without a more representative
simulations dataset, including the effects of ghosts and glints from
stars off the frame we cannot address this question.

\item No more than 0.1\% of the data can require human intervention.

We did not interfere with any of the data, but again, there was not
enough test data to find the bizarre cases and we were not forced
to limit memory

\end{itemize}



