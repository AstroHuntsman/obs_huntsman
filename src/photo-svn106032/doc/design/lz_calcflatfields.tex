\def\cfrm{{\bf correct-frames\ }}
\def\icfrm{{\bf calculate flatfields\ }}
\subsection{Introduction}

The \cfrm code will be responsible
for removing the CCD signature from the SDSS image data. It will
take decompressed raw data as input and will return fully
corrected data (\it i.e.\rm, debiased, flat-fielded, column-corrected) 
in virtual memory. The initialization
must be performed once for each CCD in the imaging camera (i.e., we
will need to run the \icfrm module 30 times to process one night
worth of data). The tasks performed by \icfrm include: 

\begin{itemize}

\item Load the quartile array data
\item Load the bias array data
\item Load the CCD defect database 
\item Compute the corrected mode array
\item Compute the normalized, inverse flat-field array (output)
\item Compute the corrected de-bias vector (output)
\item Compute the bias drift array (output)
\item Compute sky brightness variations (output)
\item Create a random byte look up table in memory (virtual output)

\end{itemize}

All 2D arrays are indexed as {\bf ARRAY[r][c]} where {\bf r} is
the row number and {\bf c} is the column number, unless 
otherwise specified. The [row][col] index order is the most efficient
access sequence in C, which uses row-oriented memory ({\it i.e.},
adjacent memory addresses are aligned with the rows, not the
columns. The opposite convention is used in FORTRAN).
It is assumed that the columns are parallel to the scan direction (see
focal plane design; Stoughton 93/3/26). 

\subsection{Input}

The reduction of the sky survey image data assumes that
the scan data will be broken into image ``frames"
which will be processed either sequentially or in parallel.
Note that while specific image dimensions are given in some
cases, proper code function does not explicitly depend on
these dimensions --- all critical arrays will either be
dynamically allocated or will have memory declarations that
are large enough to handle all plausible cases.
The \icfrm module described here assumes the following input
will be provided:

\begin{itemize}

\item The quartile array for the given CCD for the entire night.
This is the data produced in real-time by the DAQ system. The size
of the array will be 2128 columns $\times$ $\sim$1000 rows $\times$ 3
(for the 25th, 50th, and 75th percentiles). 
It is currently assumed that the quartile array WILL
include statistics for the overscan regions. 
The row dimension is determined by the frequency of quartile measurements
and the duration of the night. It is currently expected that the
quartile statistics will be generated in sync with the data frames. If
each frame is 2128 columns $\times$ 1500 rows (see \cfrm document), then
one quartile vector will be generated about every 42 seconds --- sufficient
to accurately map sky brightness variations during the night.
It is not clear yet whether the quartile array data
will be ints or shorts, and by how much they will be scaled. A 5 bit scale
factor has been proposed, which lets the sky be up to 2048 DN or
12000 electrons. The flat-field vector is generated by the 
\icfrm module from these data.

\item The bias array for the given CCD 
(size 2128 columns $\times$ $\sim1000$ rows).
In an ideal device, this would be a constant-valued array.
However, experiments with an existing Tektronics (TEK) CCD show that there
is structure in the bias vector which must subtracted out of the
image data. The data are unsigned short integers. 
The row dimension is set so that there will be enough points
per column ($\sim1000$) to yield a statistically robust measurement.
The bias array will be generated on a periodic basis during daytime
or twilight by running the survey CCDs with zero illumination. 

\item A data (or allocated tree) structure
containing the locations and the nature
of all bad columns. For each CCD, the bad column locations will be
specified by the starting column number, the total number of columns
affected, and a processing flag which indicates the type of correction
appropriate.

\end{itemize}

The total amount of data input to the \icfrm module is approximately
17 Mbytes if the quartile data are 2 byte integers or 
approximately 29 Mbytes if the quartile data are 4 byte integers. 

\subsection{Output}

The nominal output from the {\icfrm} module is

\begin{itemize}

\item The corrected, bias vector (2128 cols $\times$ 1 row).
\item The normalized, inverse flat-field array (2048 cols $\times$ $\sim1000$
rows).
\item The overscan drift correction array (1 value for each frame).
\item The sky brightness levels, adjusted to central exposure values
(1 value for each frame). 
\item A $\sim1$ Mbyte look-up table of random numbers for dithering
(kept in virtual memory).

\end{itemize}

The total output sums to just over 5 Mbyte (the output volume is
dominated by the flat-field array). It is proposed that the first
4 output items be stored to disk. 

\subsection{Modules and Algorithms}

The following sections describe those computations which
need only be done once per scan per CCD.

\subsubsection{CCD Defects and the Defect Database}

A database of CCD defect types and locations will be read in during
the \icfrm module. The \icfrm module will use this information
to properly prepare the bias and flat-field vectors.
There are basically two different kinds of CCD
defects, multiplicative ones and (always approximately) additive ones.
In some sense the variation of response over the chip is a `defect' but
one which is dealt with by the flattening process.
 
Multiplicative defects (partially blocked columns) are not defects
from the point of view of the flat-field procedure at all, unless they are
sufficiently bad that they need to be interpolated over; a partial
interpolation may be preferable; i.e., leave what is there and fill in the
rest.  In TDI mode, the relative response is just the fraction of active
pixels in the column.  These defects are common in other devices, but
do not appear to be so for TEK devices, so the problem
may be moot.  Instead, the TEK chips tend to have deep traps (probably
the same physical phenomenon) but they are eventually sated
and the response returns to normal.  Since at each vertical transfer, a
certain amount of charge decays from the trap (which can be thought of
as a metastable repository for a certain number of electrons) and is
replenished by the charge going by, in the steady state the trap has NO
effect, not even one on the charge transfer efficiency.  Whether this
will be realized in practice with the low charge levels we will
encounter with the SDSS is not clear; we may have to interpolate over
columns with really bad traps.  
 
Additive defects are not automatically removed during flattening and
must be dealt with by modifying the {\tt baseline[col] } array or
by interpolation. Examples of additive defects include
 
\begin{itemize}

\item Depressed columns --- These will appear in the defect list. The
amplitude is found from mode array, fixed in cleaned mode array and
placed in baseline vector.
 
\item Transfer gate trap column pairs. 
This is a defect in which a small amount of charge
is stolen from one column and `given back' to the following column. They
are, apparently, unique to the TEK chips. They will appear in the
defect list. The amplitude is found from mode array, fixed in cleaned
mode array and placed in baseline vector.
 
\item Low-level dark current columns. Handled as above, but the interquartile
range will be checked and an error flag raised if excessive; if so,
the column will be marked for interpolation.

\end{itemize}

Lastly there are simply BAD columns. These include intermittent dark 
current columns, persistent bad CTE columns, 
and steady dark current columns so hot
that the shot noise in the dark current is unacceptable.
These are interpolated over in data and cleanmode array.
 
The defects are stored in an array of structs for each chip which will
look something like
 
\begin{verbatim}

struct ccd_defect_t{
    short ccd_d_column;     /* Location of defect - MUST increase along 
                                whatever traversal is used  */
    short ccd_d_expamp;     /* amplitude from test data; used for comparison */
    u_short ccd_d_type;     /* type flag */
    u_char ccd_d_ncoll;     /* # of bad cols LEADING this col;
                                this is for interpolation purposes only */
    u_char ccd_d_ncolf;     /* # of bad cols FOLLOWING this col;
                                this is for interpolation purposes only */
}
 
/* Defect types:  */

#define DCCOL        1      /* low-level dark current column */
#define BLKCOL       2      /* real blocked column, just flat field */
#define BLKICOL      4      /* real badly blocked column, fill in */
#define DEPCOL       8      /* depressed column */
#define TGCOLPR     16      /* transfer-gate trap column pair; mark BOTH */
#define HOTCOL      32      /* BAD dark-current column */
#define CTECOL      64      /* BAD CTE column */
#define INTRMDCOL  128      /* BAD intermittent dark column */
 
#define BADCOL   HOTCOL|CTECOL|INTRMDCOL    /* interpolate over */
#define ADDCOL   DEPCOL|TGCOLPR|DCCOL       /* fix in biasline */
 
\end{verbatim}

The nature of most defects should be well-established by the end
of the test year (of course, the bad column database will need to
be updated if CCDs are replaced during survey operations).
For columns requiring interpolation, a linear interpolation
should be sufficient (but this could be software switchable
if there proves to be a need for anything more complex).
It is expected that most defects will only be 1 or 2 columns wide.
All column repair (including multiplicative corrections)
MUST be recorded via a modification of the associated pixel mask.

\subsubsection{Computing the Baseline Vector}
 
In many CCDs, the overscan bias level is independent
of row or column and, consequently, a very accurate
base level amplitude can be computed by averaging over
the entire overscan area. Laboratory experiments with an existing
2088 $\times$ 2048 TEK CCD show that the variation in the
overscan bias level with row is indeed negligible, although
there appears to be some dependence of the bias level
on overscan column. There is a small assymetry between the
left and right bias levels. However, the left and right bias levels
closest to the active columns are in very good agreement.
Whether this is an artifact of the lab setup or a real
intrinsic property of the CCD remains to be seen. Note that the TEK 
2080 $\times$ 2048 CCD
at Kitt Peak National Observatory does not show such structure. 
We must therefore assume
that the SDSS telescope electronics will perform significantly better
than those in the lab. 
There is also variation seen in the bias levels of the active columns
in the lab CCD. Hence, a baseline vector is required to correct
the raw data.  

The straight forward approach to generating the baseline
vector is to first clean the input bias array of ``hot" columns, which
are the only defects which should appear in these data (locations
of these columns will be accessed through a memory resident database).
Simple linear interpolation will be sufficient for the repairs.
A 1D median bias vector (2128 columns $\times$ 1 row) is then generated
by computing the median signal for each column. This bias median vector is
equivalent to the bias mode vector because the noise
is Gaussian ({\it i.e.}, mode = median). We will need to scale
up these data by at least 32 for precision
(some experimentation will be needed to determine if 32 is sufficient).
Integral powers of 2 are desirable scaling factors as then bit shifts can
be used in lieu of multiplications (which are slower). However, the
algorithms described herein should be general enough to operate with
any scale factor. Let us, hereafter, refer to this vector as the
uncorrected baseline vector, because the corrections needed
to remove additive defects in the CCD have not yet been made.

To make the ``corrected" baseline vector we need to identify additive
defects in the mode array derived from the quartile data. This can
be accomplished by computing a running mean on scales larger than the
scale of the additive defects. Significant deviations are
then measured and stored in a deviation array, hereafter referred to
as the {\tt dev[col]} vector. The {\tt dev[col]} vector is then
added to the uncorrected baseline vector to produce the corrected baseline
vector. The {\tt dev[col]} vector will be mostly filled with zeros, of course. 
The corrected baseline vector is hereafter referred to
simply as the {\tt baseline[col]} vector.
It is proposed that the {\tt baseline[col]} vector be stored on
disk for future access --- this will guard against having to rerun
the \icfrm module in the event of a system failure during
\cfrm batch processing.

Lastly, in order to track any variation in the overscan bias
level of each CCD with time, it is necessary to compute an
array containing scaled differences between the overscan
levels in the quartile array
and the mean overscan level in the {\tt baseline[col]} vector.
In principle, this could be done in the \cfrm module. However,
as we want to be able to process bright star image cutouts
with the \cfrm, in addition to standard size image frames,
it is essential that we have the overscan drift corrections
computed before the pipeline begins as the cutout images
will not have overscan data.

One problem is that, with the proposed gain setting
of 6 electrons/DN and a readnoise level of 4 electrons, the bias signal 
will be undersampled. Retrieval of the true baseline signal
may require the fitting of Gaussians to the bias data. This issue
will require further investigation.

\subsubsection{Computing the Corrected Mode Array}
 
The quartile array contains
data for the 25th, 50th, and 75th percentiles 
in each column. These data allow 
us to generate a mode array corrected for the small
skewness introduced into the sky histogram
due to the objects in the images. 
Assuming the skewness will be relatively small (objects
occupy a relatively small fraction of all pixels), then
an approximate correction can be applied to the median
as follows:

\begin{verbatim}

for (R = 0; R < NMODEROWS; R++) 
   for (c = 0; c < NCOL; c++)
      q[R][c] = 
         (3 * qtile[R][c][1] - qtile[R][c][0] - qtile[R][c][2]) << 5;

\end{verbatim}

\noindent where NMODEROWS is the total number of rows in the
quartile array, qtile[R][c][k]; NCOL is the number of columns;
and {\tt q[R][c]} is the corrected mode array. Note that {\tt q[R][c]}
is not corrected for defects --- that happens later.
The 25th, 50th, and 75th percentile values are stored in
qtile[R][c][0], qtile[R][c][1], and qtile[R][c][2],
respectively. Note that the corrected mode array is scaled up
by 32 for precision (this is statistically adequate since 
NMODEROWS $\sim 1000$). In the C language, 
{\tt <<} and {\tt >>} are bit shift
operators ({\it e.g.}, {\tt <<5} multiplies the operand by $2^{5}$;
{\tt >>10} divides the operand by $2^{10}$). 

Naturally, there are more sophisticated mode correction schemes
({\it e.g.}, fitting a 3-term Gram-Charlier series to the data)
but they can also be less robust and more cpu intensive.
Since we know our sky histograms will deviate only slightly
from Gaussian, the above method will work well. We
can certainly include the options to run a more sophisticated
mode correction algorithm and to print out the average
value of the mode correction term,
{\tt q[R][c] - qtile[R][c][1]}, as a diagnostic.
In fact, it may be desirable to have the \icfrm module look
at the mode correction value and objectively decide if 
the assumption of near-Gaussian behavior is indeed valid
(by comparing it to the expected $\sigma_{sky}$, for instance).

\subsubsection{Computing Overscan Drift Corrections}

We can compute the overscan drift corrections as follows:

\begin{verbatim}

for (R = 0; R < NMODEROWS; R++) {
   ovdelta[R] = 0;
   for (k = n = 0; k < NUMOVREG; k++) {
      for (c = OVSCOL[k]; c < OVNCOL[k]+OVSCOL[k]; c++) {
          ovdelta[R] += (q[R][c] - baseline[c]);
	  n++;
      }
   }
   ovdelta[R] /= n;
}

\end{verbatim}

\noindent where  NUMOVREG, OVSCOL[], and OVNCOL[] are the
number of overscan regions per CCD (2), the starting column
of each overscan region, and the number of columns in each
overscan region, respectively. The arrays
{\tt q[R][c]} and {\tt baseline[c]} are the 
mode and bias vectors computed previously. The output vector,
{\tt ovdelta[R]}, contains the mean difference between the
overscan level in the quartile array and that in the baseline
vector for each frame. The {\tt ovdelta[R]} array will allow
the \cfrm module to always have access to the
drift correction even if the input raw image does not have
overscan data, as is the case with image cutouts.
It is proposed that the {\tt ovdelta[R]} vector be stored on
disk for future access --- this will guard against having to rerun
the \icfrm module in the event of a system failure during
\cfrm batch processing.

\subsubsection{Computing the Flat-field Array}

A normalized, inverse flat-field vector must be created from
each row of quartile data by the \icfrm module. This will be accomplished
by 1) subtracting the cleaned baseline vector
from the corrected mode array and 2) inverting the bias-subtracted
mode array and normalizing by the mean level in the active
column region. In the following code example of how the above
procedure might be done, the cleaned mode array is called
{\tt cleanmode[c]} and the array of normalized, inverse flat-field
vectors is called {\tt FF[R][c]}:

\begin{verbatim}

long IMSCOL, IMNCOL, NMODEROWS;
long c, R, sum, skylev[];
unsigned short cleanmode[], q[][];
unsigned short baseline[], ovdelta[], FF[][];

for (R = 0; R < NMODEROWS; R++) {

   for (c = IMSCOL, sum = 0; c < IMSCOL+IMNCOL; c++) {
      cleanmode[c] = (q[R][c] - baseline[c] - ovdelta[R]);
      sum += cleanmode[c];
   }

   skylev[R] = ((sum<<6)/IMNCOL);   /* Mean value of debiased mode row */
                                    /*    scaled up by 2048.           */
                                    /* This is also the scaled array   */
                                    /*    of sky brightness levels.    */

   for (c = IMSCOL; c < IMSCOL+IMNCOL; c++) 
      FF[R][c] = skylev[R]/cleanmode[c]; 
}

\end{verbatim}

\noindent where {\tt NMODEROWS} is the total number of rows in the 
corrected mode array; 
{\tt IMSCOL} is the starting column of the active
CCD area; {\tt IMNCOL} is the number of active columns
(2048); {\tt ovdelta[R]} is the mean difference between the  
overscan level for the corrected mode array and the
overscan level in the baseline vector (this corrects for bias
drifts); and {\tt q[R][c]} is the corrected mode array.
The {\tt skylev[R]} array holds the mean sky brightness level
at each quartile measurement. The {\tt skylev[R]} array is
declared to be a long integer array to avoid overflows during
the rescaling operation.
Note that the corrected mode array ({\tt q[R][c]}), the baseline array,
and the drift correction array ({\tt ovdelta[R]}) are all scaled up 5 bits.
The resulting cleaned mode array is, thus, scaled by the same amount.
It is proposed that the {\tt FF[R][c]} and {\tt skylev[R]}
arrays be stored on
disk for future access --- this will guard against having to rerun
the \icfrm module in the event of a system failure during
\cfrm batch processing.

\subsubsection{Sky Brightness Variations} 

The sky brightness levels at the time of each quartile measurement
are stored in a 1D array (with number of elements equal to the number
of separate quartile measurements) during creation of the flat-field
vector, as shown above. They should also be
stored on disk (typically there will be $\sim1000$ sky brightness
values per CCD per night --- or $\sim30,000$ short integer values per night).
Prior to storing these data, the values should be corrected to correspond
to the central exposure time for each CCD (done by interpolation). This
will prepare the sky level data for use by subsequent modules.

\subsubsection{Random Byte Table}

We will need to add random numbers to the data during the flat-fielding
procedure to suppress data quantization effects introduced by the use
of integers. While the random numbers can be generated
on the fly in \cfrm module, it is more efficient to generate a large
($\sim1$ Mbyte) lookup table of random short integers with the appropriate
probability distribution in advance. The \icfrm module is the ideal
location to create this look-up table. We will hereafter refer to this
lookup table as {\tt DITHER[]}. The details of the probability distribution
are given in the \cfrm document.

